From: kax@cs.nott.ac.uk (Kevin Anthoney)
Subject: Re: Consciousness part II - Kev Strikes Back!
Organization: Nottingham University
Lines: 102

In article <1993Apr17.045559.12900@ousrvr.oulu.fi>
kempmp@phoenix.oulu.fi (Petri Pihko) writes:

>Kevin Anthoney (kax@cs.nott.ac.uk) wrote:
>
>: This post is probably either brilliant or insane. Do let me know
>: which... :-)
>
>A brilliant example of using the introspective objection against 
>materialist theories of consciousness.

Diplomatic :-)

I realize I'm fighting Occam's razor in this argument, so I'll try to
explain why I feel a mind is necessary. 

Firstly, I'm not impressed with the ability of algorithms. They're
great at solving problems once the method has been worked out, but not
at working out the method itself.

As a specific example, I like to solve numerical crosswords (not the
simple do-the-sums-and-insert-the-answers type, the hard ones.) To do
these with any efficiency, you need to figure out a variety of tricks.
Now, I know that you can program a computer to do these puzzles, but
in doing so you have to work out the tricks _yourself_, and program
them into the computer. You can, of course, 'obfuscate' the trick, and
write the program so that it is uncovered, but as far as I can see,
the trick still has to be there in some form to be discovered. Does
this mean that all the ideas we will ever have are already
pre-programmed into our brains? This is somewhat unlikely, given that
our brains ultimately are encoded in 46 chromosomes worth of genetic
material, much of which isn't used.

One way around this is to bring the environment into the equation, but
(again, as far as I can see) this still has an air of 'if you see
object X, then perform action Y,' and we don't seem to get anywhere.
The algorithm has to anticipate what it might see, and what
conclusions to draw from it's experience.

The other problem with algorithms is their instability. Not many
algorithms survive if you take out a large portion of their code, yet
people survive strokes without going completely haywire (there are
side-effects, but patients still seem remarkably stable.) Also,
neurons in perfectly healthy people are dying at an alarming rate -
can an algorithm survive if I randomly corrupt various bits of it's
code?

The next problem is the sticky question of "What is colour?" (replace
'colour' with the sensation of your choice.) Presumably, the
materialist viewpoint is that it's the product of some kind of
chemical reaction. The usual products of such a reaction are energy +
different chemicals. Is colour a mixture of these? If this is so, a
computer won't see colour, because the chemistry is different. Does an
algorithm that sees colour have a selective advantage over an
equivalent that doesn't? It shouldn't, because the outputs of each
algorithm ought to be the same in equivalent circumstances. So why do
we see colour?


>
>However, such a view is actually a nonsolution. How should minds be
>able to act as observers, feel pain and pleasure and issue
>commands any better than the brain? Moreover, how do the interactions
>occur?

A bit of idle speculation...

If I remember correctly, quantum mechanics consists of a wavefunction,
with two processes acting on it. The first process has been called
'Unitary Evolution' (or 'U'), is governed by Schroedinger's equation
and is well known. The second process, called various things such as
'collapse of the wavefunction' or 'state vector reduction' (or 'R'),
and is more mysterious. It is usually said to occur when a
'measurement' takes place, although nobody seems to know precisely
when that occurs. When it does occur, the effect of R is to abruptly
change the wavefunction.

I envisage R as an interaction between the wavefunction and 'something
else,' which I shall imaginitively call 'part X.' It seems reasonable
to assume that _something_ causes R, although that something might be
the wavefunction itself (in which case, part X is simply the
wavefunction. Note, though, that we'd need more than U to explain R.)

Anyway, I'm speculating that minds would be in part X. There seems to
be some link between consciousness and R, in that we never see linear
superpositions of anything, although there are alternative
explainations for this. I've no idea how a brain is supposed to access
part X, but since this is only speculation, that won't matter too
much :-) My main point is that there might be a place for minds in
physics.

I'll go back to my nice padded cell now, if that's OK with you :-)

>
>
>Petri

-- 
------------------------------------------------------------------------
Kevin Anthoney                                         kax@cs.nott.ac.uk
            Don't believe anything you read in .sig files.
------------------------------------------------------------------------
